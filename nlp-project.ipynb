{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10042448,"sourceType":"datasetVersion","datasetId":6186419},{"sourceId":10058969,"sourceType":"datasetVersion","datasetId":6198591},{"sourceId":10065555,"sourceType":"datasetVersion","datasetId":6203297}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Load dataset with latin1 encoding\ndata = pd.read_csv('/kaggle/input/keyphrases/dataset (1).csv', encoding='latin1')\n\n# Clean text function\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    \n    text = str(text)\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n    text = re.sub(r'http\\S+\\.(jpg|jpeg|png|gif|bmp|svg)', '', text, flags=re.IGNORECASE)  # Remove image URLs\n    text = re.sub(r'http\\S+', '', text)  # Remove any URL\n    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # Remove bold markdown\n    text = re.sub(r'__(.*?)__', r'\\1', text)  # Remove underlined markdown\n    allowed_chars_pattern = r'[^a-zA-Z0-9.,?!:;+=\\-\\*/()\\[\\]{} ]+'  # Remove unwanted characters\n    text = re.sub(allowed_chars_pattern, '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n    return text\n\n# Check if 'question' and 'answer' columns exist, then clean them\nif 'questions' in data.columns and 'answers' in data.columns:\n    data['questions'] = data['questions'].apply(clean_text)\n    data['answers'] = data['answers'].apply(clean_text)\nelse:\n    print(\"The dataset does not contain 'question' and 'answer' columns.\")\n\n# Adjust pandas display options to show the entire dataset\npd.set_option('display.max_rows', None)  # Set this to None to show all rows\npd.set_option('display.max_columns', None)  # Set this to None to show all columns\npd.set_option('display.width', None)  # Avoid line breaks in output\npd.set_option('display.max_colwidth', None)  # Show full column content without truncation\n\n# Display the DataFrame\nfrom IPython.display import display\ndisplay(data.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:22.027997Z","iopub.execute_input":"2024-12-10T11:02:22.028415Z","iopub.status.idle":"2024-12-10T11:02:22.095190Z","shell.execute_reply.started":"2024-12-10T11:02:22.028380Z","shell.execute_reply":"2024-12-10T11:02:22.093895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import bigrams\nfrom nltk.tokenize import word_tokenize\nimport string\nimport pandas as pd\n\n# Download NLTK stopwords if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Define stop words\nstop_words = set(stopwords.words('english'))\n\n# Function to clean and tokenize text\ndef clean_and_tokenize(text):\n    # Ensure the text is a string (if it's None, NaN, or other types, return an empty list)\n    if not isinstance(text, str):\n        return []\n    \n    # Tokenize, including words with hyphen\n    tokens = word_tokenize(text)\n    \n    # Remove stop words and punctuation, but keep words with hyphens\n    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n    \n    return tokens\n\n# Function to generate bigrams or fall back to unigrams, and filter out duplicate consecutive words\ndef generate_bigrams_or_unigrams(tokens):\n    if len(tokens) < 2:\n        # If less than two tokens, return unigrams (single tokens)\n        return [token for token in tokens]  # Return as individual tokens\n    \n    # Generate bigrams\n    bigram_list = list(bigrams(tokens))\n    \n    # Remove bigrams where both words are the same\n    bigram_list = [[bigram[0], bigram[1]] for bigram in bigram_list if bigram[0] != bigram[1]]\n    \n    return bigram_list\n\n# Assuming `data` is your actual DataFrame (replace this with your actual DataFrame if not defined)\n# Replace the 'question' column with bigrams (or unigrams if bigrams are not possible)\ndata['question_tokens'] = data['questions'].apply(lambda x: generate_bigrams_or_unigrams(clean_and_tokenize(x)))\n\n# Show the first few rows of the dataframe with the updated 'question' column (bigrams or unigrams) and the 'answer' column\nfrom IPython.display import display\ndisplay(data[['questions','question_tokens', 'answers']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:22.097625Z","iopub.execute_input":"2024-12-10T11:02:22.098100Z","iopub.status.idle":"2024-12-10T11:02:22.167508Z","shell.execute_reply.started":"2024-12-10T11:02:22.098052Z","shell.execute_reply":"2024-12-10T11:02:22.166291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef compute_cosine_similarity(user_keyphrases, question_keyphrases):\n    # Ensure input keyphrases are not empty\n    if not user_keyphrases or not question_keyphrases:\n        return 0  # No similarity if either list is empty\n\n    # Join keyphrases into a single string\n    user_str = ' '.join(user_keyphrases)\n    question_str = ' '.join(question_keyphrases)\n\n    # Ensure strings are not empty\n    if not user_str.strip() or not question_str.strip():\n        return 0  # No similarity if strings are empty\n\n    # Compute cosine similarity\n    vectorizer = CountVectorizer()\n    vectors = vectorizer.fit_transform([user_str, question_str])\n    similarity_matrix = cosine_similarity(vectors)\n    return similarity_matrix[0][1]  # Return similarity score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:22.168922Z","iopub.execute_input":"2024-12-10T11:02:22.169288Z","iopub.status.idle":"2024-12-10T11:02:22.176425Z","shell.execute_reply.started":"2024-12-10T11:02:22.169248Z","shell.execute_reply":"2024-12-10T11:02:22.175164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure 'questions_keyphrases' is a list of tokens\ndata['questions_keyphrases'] = data['questions_keyphrases'].apply(\n    lambda x: x if isinstance(x, list) else x.split()\n)\n\n# Remove rows where 'questions_keyphrases' is empty\ndata = data[data['questions_keyphrases'].apply(len) > 0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:22.178534Z","iopub.execute_input":"2024-12-10T11:02:22.178956Z","iopub.status.idle":"2024-12-10T11:02:22.201140Z","shell.execute_reply.started":"2024-12-10T11:02:22.178899Z","shell.execute_reply":"2024-12-10T11:02:22.199702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\ndef clean_text(text):\n    \"\"\"Clean text by removing non-alphabetic characters, lowercasing, and stripping extra spaces.\"\"\"\n    text = text.lower()  # Lowercase the text\n    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Remove non-alphanumeric characters\n    return text.strip()\n\ndef check_completeness(candidate_answer, key_phrases):\n    key_text = \" \".join(key_phrases)  # Combine key phrases into one string\n    candidate_answer_cleaned = clean_text(candidate_answer)  # Clean candidate answer\n    key_text_cleaned = clean_text(key_text)  # Clean key phrases text\n    tfidf = TfidfVectorizer().fit_transform([candidate_answer_cleaned, key_text_cleaned])  # Apply TF-IDF\n    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])  # Calculate cosine similarity\n    return similarity[0][0]  # Return similarity score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:22.202773Z","iopub.execute_input":"2024-12-10T11:02:22.203336Z","iopub.status.idle":"2024-12-10T11:02:22.218147Z","shell.execute_reply.started":"2024-12-10T11:02:22.203282Z","shell.execute_reply":"2024-12-10T11:02:22.216981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\n# Initialize the model once\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef evaluate_accuracy(candidate_answer, correct_answer):\n    # Check if the answers are valid strings\n    if not isinstance(candidate_answer, str) or not isinstance(correct_answer, str):\n        raise ValueError(\"Both candidate_answer and correct_answer must be strings.\")\n    \n    # Encode both answers into embeddings\n    embeddings = model.encode([candidate_answer, correct_answer], convert_to_tensor=True)\n    \n    # Calculate cosine similarity between the two embeddings\n    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n    return similarity.item()  # Return the similarity as a scalar value\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:22.220036Z","iopub.execute_input":"2024-12-10T11:02:22.220502Z","iopub.status.idle":"2024-12-10T11:02:23.383654Z","shell.execute_reply.started":"2024-12-10T11:02:22.220452Z","shell.execute_reply":"2024-12-10T11:02:23.382526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install textstat\nimport textstat\n\ndef evaluate_clarity(answer):\n    score = textstat.flesch_reading_ease(answer)\n    return score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:23.387131Z","iopub.execute_input":"2024-12-10T11:02:23.387665Z","iopub.status.idle":"2024-12-10T11:02:33.692761Z","shell.execute_reply.started":"2024-12-10T11:02:23.387615Z","shell.execute_reply":"2024-12-10T11:02:33.691253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sentence-transformers\nimport spacy\n\ndef evaluate_logical_flow(answer):\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(answer)\n    sentence_count = len(list(doc.sents))\n    avg_sentence_length = sum(len(sent) for sent in doc.sents) / sentence_count\n    return {\"sentence_count\": sentence_count, \"avg_sentence_length\": avg_sentence_length}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:33.694582Z","iopub.execute_input":"2024-12-10T11:02:33.694955Z","iopub.status.idle":"2024-12-10T11:02:44.068795Z","shell.execute_reply.started":"2024-12-10T11:02:33.694921Z","shell.execute_reply":"2024-12-10T11:02:44.067485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Example DataFrame (replace with your actual dataset)\n# Ensure `data` has 'questions', 'answers', 'questions_keyphrases', 'answers_keyphrases' columns\n# Example loading:\n# data = pd.read_csv(\"your_dataset.csv\")\n\n# Function to compute cosine similarity\ndef compute_cosine_similarity(user_keyphrases, question_keyphrases):\n    user_str = ' '.join(user_keyphrases)\n    question_str = ' '.join(question_keyphrases)\n    vectorizer = CountVectorizer().fit_transform([user_str, question_str])\n    similarity_matrix = cosine_similarity(vectorizer)\n    return similarity_matrix[0][1]\n\n# Ensure questions_keyphrases are in list format\ndata['questions_keyphrases'] = data['questions_keyphrases'].apply(\n    lambda x: x if isinstance(x, list) else x.split()\n)\n\n# User's input\nuser_intro = \"supervised machine learning\"\nuser_intro_keyphrases = user_intro.split()  # Replace with keyphrase extraction logic\n\n# Compute similarities for all questions\nsimilarities = data['questions_keyphrases'].apply(\n    lambda x: compute_cosine_similarity(user_intro_keyphrases, x)\n)\n\n# Find the most similar question\nmost_similar_idx = similarities.idxmax()\n\n# Evaluate depth for the corresponding answer\ndef evaluate_depth_with_keyphrases(answer, extracted_keyphrases):\n    if not extracted_keyphrases or not isinstance(extracted_keyphrases, list):\n        return {\"word_count\": len(answer.split()), \"keyphrase_coverage\": 0, \"missing_keyphrases\": []}\n    \n    answer_lower = answer.lower()\n    covered_keyphrases = [kw for kw in extracted_keyphrases if kw.lower() in answer_lower]\n    keyphrase_coverage = len(covered_keyphrases) / len(extracted_keyphrases) if extracted_keyphrases else 0\n    missing_keyphrases = [kw for kw in extracted_keyphrases if kw.lower() not in answer_lower]\n    \n    return {\n        \"word_count\": len(answer.split()),\n        \"keyphrase_coverage\": keyphrase_coverage,\n        \"covered_keyphrases\": covered_keyphrases,\n        \"missing_keyphrases\": missing_keyphrases\n    }\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:44.071026Z","iopub.execute_input":"2024-12-10T11:02:44.071427Z","iopub.status.idle":"2024-12-10T11:02:44.356427Z","shell.execute_reply.started":"2024-12-10T11:02:44.071391Z","shell.execute_reply":"2024-12-10T11:02:44.355299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import language_tool_python\n\n# Function to calculate Grammatical Accuracy\ndef calculate_grammatical_accuracy(text):\n    \"\"\"Calculate the grammatical accuracy of the text.\"\"\"\n    tool = language_tool_python.LanguageTool('en-US')\n    matches = tool.check(text)\n    num_errors = len(matches)\n    total_words = len(text.split())\n    \n    if total_words == 0:\n        return 1.0  # Avoid division by zero if no words\n    \n    error_rate = num_errors / total_words\n    grammatical_accuracy = 1 - error_rate\n    \n    return grammatical_accuracy, num_error\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:02:44.358077Z","iopub.execute_input":"2024-12-10T11:02:44.358548Z","iopub.status.idle":"2024-12-10T11:02:44.367423Z","shell.execute_reply.started":"2024-12-10T11:02:44.358501Z","shell.execute_reply":"2024-12-10T11:02:44.365904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef evaluate_answer(user_answer, correct_answer, keyphrases):\n    \"\"\"Evaluate the answer based on various metrics.\"\"\"\n    \n    # Ensure the answers are not empty\n    if not user_answer.strip() or not correct_answer.strip():\n        print(\"One of the answers is empty. Returning default scores.\")\n        return {\n            'accuracy': 0.0,\n            'completeness': 0.0,\n            'clarity': 0.0,\n            'logical_flow': 0.0\n        }\n    \n    # Calculate Accuracy using Cosine Similarity\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform([user_answer, correct_answer])\n    accuracy = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n\n    # Calculate Completeness as the percentage of key phrases mentioned\n    keyphrases_count = sum(1 for kp in keyphrases if kp in user_answer.split())\n    completeness = keyphrases_count / len(keyphrases) if keyphrases else 0\n\n    # Calculate Clarity (you can add a more advanced method, for now, it's a placeholder)\n    clarity = 1.0  # Assuming a perfect clarity score for simplicity\n\n    # Logical Flow (you can implement this as needed)\n    logical_flow = 1.0  # Placeholder for logical flow analysis\n    \n    return {\n        'accuracy': accuracy,\n        'completeness': completeness,\n        'clarity': clarity,\n        'logical_flow': logical_flow\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:03:46.873956Z","iopub.execute_input":"2024-12-10T11:03:46.874441Z","iopub.status.idle":"2024-12-10T11:03:46.883743Z","shell.execute_reply.started":"2024-12-10T11:03:46.874401Z","shell.execute_reply":"2024-12-10T11:03:46.882502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_score(total_weighted_grade, max_possible_score):\n    \"\"\"\n    Normalize the total weighted grade to a 0-100 scale.\n\n    Parameters:\n    - total_weighted_grade (float): The calculated weighted grade.\n    - max_possible_score (float): The maximum possible score based on weights.\n\n    Returns:\n    - float: The normalized grade on a 0-100 scale.\n    \"\"\"\n    try:\n        return (total_weighted_grade / max_possible_score) * 100\n    except ZeroDivisionError:\n        return 0  # Return 0 if the max_possible_score is 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:03:47.848084Z","iopub.execute_input":"2024-12-10T11:03:47.848574Z","iopub.status.idle":"2024-12-10T11:03:47.855135Z","shell.execute_reply.started":"2024-12-10T11:03:47.848535Z","shell.execute_reply":"2024-12-10T11:03:47.853531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n    \n# Calculate Grammatical Accuracy\n#grammatical_accuracy, _ = calculate_grammatical_accuracy(answer_text)\n\ndef calculate_weighted_grade(evaluation, answer_text):\n    \"\"\"Calculate the weighted grade based on the evaluation metrics and grammatical accuracy.\"\"\"\n    try:\n        accuracy_score = float(evaluation.get('accuracy', 0))  # Default to 0 if missing\n        completeness_score = float(evaluation.get('completeness', 0))\n        clarity_score = float(evaluation.get('clarity', 0))\n        grammatical_accuracy = float(evaluation.get('grammatical_accuracy', 0))\n\n    except Exception as e:\n        print(f\"Error in conversion: {e}\")\n        return 0  # Return a default value in case of conversion failure\n\n    \n    # Debug: Print the raw evaluation scores\n    print(f\"Raw Scores - Accuracy: {accuracy_score}, Completeness: {completeness_score}, \"\n          f\"Clarity: {clarity_score}, Grammatical Accuracy: {grammatical_accuracy}\")\n    \n    # Define weights for each metric (adjusted for grammatical accuracy)\n    weights = {\n        'accuracy': 0.45,   # Reduced to accommodate grammatical accuracy\n        'completeness': 0.25,\n        'clarity': 0.1,\n        'grammatical_accuracy': 0.2  # Added grammatical accuracy with 0.2 weight\n    }\n    \n    # Weighted score for each metric\n    weighted_accuracy = accuracy_score * weights['accuracy']\n    weighted_completeness = completeness_score * weights['completeness']\n    weighted_clarity = clarity_score * weights['clarity']\n    weighted_grammatical_accuracy = grammatical_accuracy * weights['grammatical_accuracy']\n    \n    # Calculate total weighted grade\n    total_weighted_grade = (weighted_accuracy + weighted_completeness + weighted_clarity + weighted_grammatical_accuracy)\n    \n    # Debug: Print the weighted score before normalization\n    print(f\"Weighted Total Score: {total_weighted_grade}\")\n    \n    # Normalize the score to a 0-100 scale (assuming max possible score is 1 for each metric)\n    max_possible_score = sum(weights.values())  # 1 for each metric, based on 100% weightage\n    normalized_grade = normalize_score(total_weighted_grade, max_possible_score)\n    \n    # Debug: Print the normalized score\n    print(f\"Normalized Grade: {normalized_grade}\")\n    \n    return normalized_grade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:03:48.295572Z","iopub.execute_input":"2024-12-10T11:03:48.296039Z","iopub.status.idle":"2024-12-10T11:03:48.306652Z","shell.execute_reply.started":"2024-12-10T11:03:48.296002Z","shell.execute_reply":"2024-12-10T11:03:48.305041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def automatic_interview():\n    visited = set()  # Keep track of visited questions\n    evaluation_results = []  # Store evaluation metrics\n    total_questions = 10\n    user_intro = \"\"\n    weighted_grades = []  # List to store final grades for each question\n    \n    # Ask 2-3 self-introduction questions first (but mix in variety)\n    intro_questions = [\n        \"Tell me about yourself.\",\n        \"What are your key strengths?\",\n        \"What is your educational background?\"\n    ]\n    \n    random.shuffle(intro_questions)  # Shuffle the intro questions\n    for i, question in enumerate(intro_questions):\n        if i >= 3:  # Limit to 2-3 intro questions\n            break\n        print(f\"Q{i+1}: {question}\")\n        answer = input(\"Your Answer: \")\n        visited.add(question)  # Mark question as visited\n        evaluation_results.append(evaluate_answer(answer, \"\", []))  # Evaluate with no keyphrases initially\n        user_intro += f\" {answer}\"  # Build user introduction context\n    \n    # Tokenize user intro into keyphrases\n    user_intro_keyphrases = user_intro.split()\n    \n    # Continue with dataset questions, ensuring variety and relatedness to previous responses\n    for i in range(len(intro_questions), total_questions):\n        # Compute similarities for unvisited questions\n        data['similarity'] = data['questions_keyphrases'].apply(\n            lambda x: compute_cosine_similarity(user_intro_keyphrases, x)\n        )\n        \n        # Select the most similar unvisited question (avoiding repetition)\n        unvisited_data = data[~data['questions'].isin(visited)]\n        if unvisited_data.empty:\n            print(\"No more unvisited questions available.\")\n            break\n        \n        most_similar_idx = unvisited_data['similarity'].idxmax()\n        selected_question = data.loc[most_similar_idx, 'questions']\n        selected_keyphrases = data.loc[most_similar_idx, 'answers_keyphrases']\n        correct_answer = data.loc[most_similar_idx, 'answers']\n        \n        print(f\"Q{i+1}: {selected_question}\")\n        answer = input(\"Your Answer: \")\n        \n        # Mark the question as visited\n        visited.add(selected_question)\n        \n        # Evaluate the answer\n        evaluation = evaluate_answer(answer, correct_answer, selected_keyphrases)\n        \n        # Debug: Print evaluation result (inspect the structure of the evaluation dictionary)\n        print(f\"Evaluation Result for Question {i+1}: {evaluation}\")\n        \n        evaluation_results.append(evaluation)\n        \n        # Calculate the weighted grade and store it\n        weighted_grade = calculate_weighted_grade(evaluation, answer)\n        weighted_grades.append(weighted_grade)\n        \n        # Add answer to user context (this will help diversify next question's selection)\n        user_intro += f\" {answer}\"\n        user_intro_keyphrases = user_intro.split()  # Update user intro keyphrases\n    \n    print(\"\\nInterview Completed. Evaluation Results:\")\n    \n    # Only print weighted grades for the answers that have been evaluated\n    for i, result in enumerate(evaluation_results):\n        print(f\"Q{i+1} - Metrics: {result}\")\n        if i < len(weighted_grades):  # Ensure we do not go out of range\n            print(f\"Q{i+1} - Weighted Grade: {weighted_grades[i]:.2f}\")\n    \n    # Calculate the overall score (normalized to 100)\n    overall_score = sum(weighted_grades) / len(weighted_grades) if weighted_grades else 0\n    overall_score_normalized = normalize_score(overall_score, 100)\n    print(f\"\\nOverall Score: {overall_score_normalized/10:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:03:48.877325Z","iopub.execute_input":"2024-12-10T11:03:48.877798Z","iopub.status.idle":"2024-12-10T11:03:48.890572Z","shell.execute_reply.started":"2024-12-10T11:03:48.877763Z","shell.execute_reply":"2024-12-10T11:03:48.889070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the automatic interview\nautomatic_interview()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:05:19.340921Z","iopub.execute_input":"2024-12-10T11:05:19.341304Z","iopub.status.idle":"2024-12-10T11:07:43.361421Z","shell.execute_reply.started":"2024-12-10T11:05:19.341272Z","shell.execute_reply":"2024-12-10T11:07:43.360188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}